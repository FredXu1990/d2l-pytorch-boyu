# d2l-pytorch-boyu
《动手学深度学习》pytorch版本学习打卡

# 第一次打卡：2020-2-14

## Task1：线性回归；Softmax与分类模型；多层感知机

- 在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。 

- softmax将输出值变换成值为正且和为1的概率分布

- 交叉熵（cross entropy）是一个常用的衡量两个概率分布差异的测量函数。
  $$
  H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},
  $$

​    交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。交叉熵损失函数定义为 
$$
\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),
$$

- 全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。

- 关于激活函数的选择

  ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。

  用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  

  在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。

  在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。



## Task2：文本预处理；语言模型；循环神经网络基础

### 文本预处理

- 文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：

1. 读入文本

2. 分词:我们对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。

3. 建立字典，将每个词映射到一个唯一的索引（index）:为了方便模型处理，我们需要将字符串转换为数字。因此我们需要先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号;使用字典，我们可以将原文本中的句子从单词序列转换为索引序列

4. 将文本从词的序列转换为索引的序列，方便输入模型

   

   一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：

   $$
   P(w_1, w_2, \ldots, w_T).
   $$


   $n$元语法（$n$-gram）是一种基于统计的语言模型。

# 第二次打卡 ：2020-2-18

## Task03：过拟合、欠拟合及其解决方案；梯度消失、梯度爆炸；循环神经网络进阶



## Task04: 机器翻译及相关技术；注意力机制与Seq2seq模型；Transformer



## Task05: 卷积神经网络基础；leNet；卷积神经网络进阶

### 卷积神经网络基础

- 卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。
- 影响元素𝑥x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做𝑥x的感受野（receptive field）
- 我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。

- 1×1卷积层：1×1卷积核可在不改变高宽的情况下，调整通道数。1×11×1卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×11×1卷积层的作用与全连接层等价。

- 卷积层与全连接层对比：一是**全连接层**把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，**网络难以捕捉局部信息**。而卷积层的设计，天然地具有提取局部信息的能力。二是卷积层的参数量更少,对于大尺寸的输入图像，使用全连接层容易导致模型过大； 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大（权重共享），使用卷积层可以以较少的参数数量来处理更大的图像。
- 池化层主要用于缓解卷积层对位置的过度敏感性,分为最大池化、平均池化。

### LeNet

LeNet分为卷积层块和全连接层块两个部分

![Image Name](https://cdn.kesci.com/upload/image/q5ndwsmsao.png?imageView2/0/w/960/h/960)

- 卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。

- 全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。

### 卷积神经网络进阶

#### AlexNet

**特征：**

1. 8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
2. 将sigmoid激活函数改成了更加简单的ReLU激活函数。
3. 用Dropout来控制全连接层的模型复杂度。
4. 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

![Image Name](https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640)

### VGGNet

VGG：通过重复使⽤简单的基础块来构建深度模型。  
Block:数个相同的填充为1、窗口形状为$3\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\times 2$的最大池化层。  
卷积层保持输入的高和宽不变，而池化层则对其减半。


![Image Name](https://cdn.kesci.com/upload/image/q5l6vut7h1.png?imageView2/0/w/640/h/640)

### 网络中的网络（NiN）

LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。  
NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。  
⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。  

![Image Name](https://cdn.kesci.com/upload/image/q5l6u1p5vy.png?imageView2/0/w/960/h/960)

1×1卷积核作用   
1.放缩通道数：通过控制卷积核的数量达到通道数的放缩。  
2.增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。  
3.计算参数少   

### GoogLeNet

1. 由Inception基础块组成。  
2. Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。   
3. 可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 

![Image Name](https://cdn.kesci.com/upload/image/q5l6uortw.png?imageView2/0/w/640/h/640)

完整模型结构

![Image Name](https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640)

# 第三次打卡 2020-2-25

## Task06：批量归一化和残差网络；凸优化；梯度下降



## Task07：优化算法进阶；word2vec；词嵌入进阶



## Task08：文本分类；数据增强；模型微调



## Task09：目标检测基础；图像风格迁移；图像分类案例1



## Task10：图像分类案例2；GAN；DCGAN



## 大作业1 卷积神经网络开放题：探索卷积神经网络在计算机视觉任务中的演化历程(CV方向)

## 大作业2 语言模型开放题：探索循环神经网络在构建语言模型中的演化历程(NLP方向)